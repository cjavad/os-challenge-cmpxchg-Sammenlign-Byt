#define b ByteswapMask
	3
	2
	1
	0
	7
	6
	5
	4
	11
	10
	9
	8
	15
	14
	13
	12

#define dw C80
	0x80000000
	0x80000000
	0x80000000
	0x80000000

#define dw C64
	64
	64
	64
	64

#define dw H
	0x6a09e667
	0x6a09e667
	0x6a09e667
	0x6a09e667
	0xbb67ae85
	0xbb67ae85
	0xbb67ae85
	0xbb67ae85
	0x3c6ef372
	0x3c6ef372
	0x3c6ef372
	0x3c6ef372
	0xa54ff53a
	0xa54ff53a
	0xa54ff53a
	0xa54ff53a
	0x510e527f
	0x510e527f
	0x510e527f
	0x510e527f
	0x9b05688c
	0x9b05688c
	0x9b05688c
	0x9b05688c
	0x1f83d9ab
	0x1f83d9ab
	0x1f83d9ab
	0x1f83d9ab
	0x5be0cd1
	0x5be0cd1
	0x5be0cd1
	0x5be0cd1

#define dw K
	0x428a2f98
	0x71374491
	0xb5c0fbcf
	0xe9b5dba5
	0x3956c25b
	0x59f111f1
    0x923f82a4
	0xab1c5ed5
	0xd807aa98
	0x12835b01
	0x243185be
	0x550c7dc3
    0x72be5d74
	0x80deb1fe
	0x9bdc06a7
	0xc19bf174
	0xe49b69c1
	0xefbe4786
    0x0fc19dc6
	0x240ca1cc
	0x2de92c6f
	0x4a7484aa
	0x5cb0a9dc
	0x76f988da
    0x983e5152
	0xa831c66d
	0xb00327c8
	0xbf597fc7
	0xc6e00bf3
	0xd5a79147
    0x06ca6351
	0x14292967
	0x27b70a85
	0x2e1b2138
	0x4d2c6dfc
	0x53380d13
    0x650a7354
	0x766a0abb
	0x81c2c92e
	0x92722c85
	0xa2bfe8a1
	0xa81a664b
    0xc24b8b70
	0xc76c51a3
	0xd192e819
	0xd6990624
	0xf40e3585
	0x106aa070
    0x19a4c116
	0x1e376c08
	0x2748774c
	0x34b0bcb5
	0x391c0cb3
	0x4ed8aa4a
    0x5b9cca4f
	0x682e6ff3
	0x748f82ee
	0x78a5636f
	0x84c87814
	0x8cc70208
    0x90befffa
	0xa4506ceb
	0xbef9a3f7
	0xc67178f2

#define b PunpackMask
	6
	4
	2
	0
	14
	12
	10
	8
	7
	5
	3
	1
	15
	13
	11
	9

#func rotr dest, src, shift
	vpsrld tmp0, $src, $shift
	vpslld tmp1, $src, 32 - $shift
	vpor $dest, tmp0, tmp1

#func s0 dest, src
	@rotr tmp0, $src, 18
	@rotr tmp1, $src, 7
	vpsrld tmp2, $src, 3
	vpxor tmp3, tmp0, tmp1
	vpxor $dest, tmp3, tmp2

#func s1 dest, src
	@rotr tmp0, $src, 19
	@rotr tmp1, $src, 17
	vpsrld tmp2, $src, 10
	vpxor tmp3, tmp0, tmp1
	vpxor $dest, tmp3, tmp2

#func S0 dest, src
	@rotr tmp0, $src, 2
	@rotr tmp1, $src, 13
	@rotr tmp2, $src, 22
	vpxor tmp3, tmp0, tmp1
	vpxor $dest, tmp3, tmp2

#func S1 dest, src
	@rotr tmp0, $src, 6
	@rotr tmp1, $src, 11
	@rotr tmp2, $src, 25
	vpxor tmp3, tmp0, tmp1
	vpxor $dest, tmp3, tmp2

#func maj dest, a, b, c
	vpand tmp0, $a, $b
	vpand tmp1, $a, $c
	vpand tmp2, $b, $c
	vpxor tmp3, tmp0, tmp1
	vpxor $dest, tmp3, tmp2

#func ch dest, e, f, g
	vpand tmp0, $e, $f
	vpandn tmp1, $e, $g
	vpxor $dest, tmp0, tmp1

#func temp1 dest, e, f, g, w, k
	@S1 tmp0, $e
	@ch tmp1, $e, $f, $g
	vpaddd tmp2, $k, $w
	vpaddd tmp3, tmp0, tmp1
	vpaddd $dest, tmp2, tmp3

#func temp2 dest, a, b, c
	@S0 tmp0, $a
	@maj tmp1, $a, $b, $c
	vpaddd $dest, tmp0, tmp1

#func compress a, b, c, d, e, f, g, h, w, k
	@temp2 tmp2, $a, $b, $c
	@temp1 tmp1, $e, $f, $g, $w, $k
	vmovdqa $h, $g
	vmovdqa $g, $f
	vmovdqa $f, $e
	vpaddd $e, $d, tmp1
	vmovdqa $d, $c
	vmovdqa $c, $b
	vmovdqa $b, $a
	vpaddd $a, tmp1, tmp2

#body
	vzeroupper
	sub %rsp 1024 + 8 - 112

	vmovdqa mask, %ByteswapMask
	vmovdqa h0, [%rsi]
	vmovdqa h1. [%rsi + 16]

	vpshufb h0, h0, mask
	vpshufb h1, h1, mask

	vpunpckldq tmp0, h0, h1
	vpunpckhdq tmp1, h0, h1
	vpunpckldq w0, tmp0, tmp1
	vpunpckhdq w1, tmp0, tmp1

	vmovdqa [%rsp - 112 + (16 * 0)], w0
	vmovdqa [%rsp - 112 + (16 * 1)], w1

	vmovdqa c80, %C80
	vmovdqa c64, %C64

	vzero z0

	vmovdqa [%rsp - 112 + (16 * 2)], c80
	vmovdqa [%rsp - 112 + (16 * 3)], z0
	vmovdqa [%rsp - 112 + (16 * 4)], z0
	vmovdqa [%rsp - 112 + (16 * 5)], z0
	vmovdqa [%rsp - 112 + (16 * 6)], z0
	vmovdqa [%rsp - 112 + (16 * 7)], z0
	vmovdqa [%rsp - 112 + (16 * 8)], z0
	vmovdqa [%rsp - 112 + (16 * 9)], z0
	vmovdqa [%rsp - 112 + (16 * 10)], z0
	vmovdqa [%rsp - 112 + (16 * 11)], z0
	vmovdqa [%rsp - 112 + (16 * 12)], z0
	vmovdqa [%rsp - 112 + (16 * 13)], z0
	vmovdqa [%rsp - 112 + (16 * 14)], z0
	vmovdqa [%rsp - 112 + (16 * 15)], c64

	// w16 = w0 + s0(w1)
	@s0 tmp0, w1
	vpaddd w16, w0, tmp0
	vmovdqa [%rsp - 112 + (16 * 16)], w16

	// w17 = w1 + s0(c80) + s1(c64)
	@s0 tmp0, c80
	@s1 tmp1, c64
	vpaddd tmp2, tmp0, tmp1
	vpaddd w17, w1, tmp2
	vmovdqa [%rsp - 112 + (16 * 17)], w17

	// w18 = c80 + s1(w16)
	@s1 tmp0, w16
	vpaddd w18 c80, tmp0
	vmovdqa [%rsp - 112 + (16 * 18)], w18

	// w19 = s1(w17)
	@s1 w19, w17
	vmovdqa [%rsp - 112 + (16 * 19)], w19

	// w20 = s1(w18)
	@s1 w20, w18
	vmovdqa [%rsp - 112 + (16 * 20)], w20

	// w21 = s1(w19)
	@s1 w21, w19
	vmovdqa [%rsp - 112 + (16 * 21)], w21

	// w22 = c64 + s1(w20)
	@s1 tmp0, w22
	vpaddd w22, tmp0, c64
	vmovdqa [%rsp - 112 + (16 * 22)], w22

	// w23 = w16 + s1(w21)
	vmovdqa w16, [%rsp - 112 + (16 * 16)]
	@s1 tmp0, w21
	vpaddd w23, w16, tmp0
	vmovdqa [%rsp - 112 + (16 * 23)], w23

	// w24 = w17 + s1(w22)
	vmovdqa w17, [%rsp - 112 + (16 * 17)]
	@s1 tmp0, w22
	vpaddd w24, w17, tmp0
	vmovdqa [%rsp - 112 + (16 * 24)], w24

	// w25 = w18 + s1(w23)
	vmovdqa w18, [%rsp - 112 + (16 * 18)]
	@s1 tmp0, w23
	vpaddd w25, w18, tmp0
	vmovdqa [%rsp - 112 + (16 * 25)], w25

	// w26 = w19 + s1(w24)
	vmovdqa w19, [%rsp - 112 + (16 * 19)]
	@s1 tmp0, w24
	vpaddd w26, w19, tmp0
	vmovdqa [%rsp - 112 + (16 * 26)], w26

	// w27 = w20 + s1(w25)
	vmovdqa w20, [%rsp - 112 + (16 * 20)]
	@s1 tmp0, w25
	vpaddd w27, w20, tmp0
	vmovdqa [%rsp - 112 + (16 * 27)], w27

	// w28 = w21 + s1(w26)
	vmovdqa w21, [%rsp - 112 + (16 * 21)]
	@s1 tmp0, w26
	vpaddd w28, w21, tmp0
	vmovdqa [%rsp - 112 + (16 * 28)], w28

	// w29 = w22 + s1(w27)
	vmovdqa w22, [%rsp - 112 + (16 * 22)]
	@s1 tmp0, w27
	vpaddd w29, w22, tmp0
	vmovdqa [%rsp - 112 + (16 * 29)], w29

	// w30 = s0(c64) + w23 + s1(w28)
	vmovdqa w23, [%rsp - 112 + (16 * 23)]
	@s1 tmp0, w28
	@s0 tmp1, c64
	vpaddd tmp2, tmp0, tmp1
	vpaddd w30, tmp2, w23
	vmovdqa [%rsp - 112 + (16 * 30)], w30

	// w31 = c64 + s0(w16) + w24 + s1(w29)
	vmovdqa w16, [%rsp - 112 + (16 * 16)]
	vmovdqa w24, [%rsp - 112 + (16 * 24)]
	@s1 tmp0, w29
	@s0 tmp1, w16
	vpaddd tmp2, tmp0, w24
	vpaddd tmp3, tmp1, c64
	vpaddd w31, tmp2, tmp3
	vmovdqa [%rsp - 112 + (16 * 31)], w31

	// setup for loop
	vmovdqa wi16, w16
	vmovdqa wi15, [%rsp - 112 + (16 * 17)]
	vmovdqa wi7, [%rsp - 112 + (16 * 25)]
	vmovdqa wi2, w30

	lea %rax, [%rsp - 112 + (16 * 32)]
	lea %rcx, [%rsp - 112 + (16 * 63)]

	vmovdqa wlater, wi31

	// loop begin
	%=:

	@s1 tmp0, wi2
	@s0 tmp1, wi15
	vpaddd tmp2, tmp0, tmp1
	vpaddd tmp3, wi16, wi7
	vpaddd wi, tmp2, tmp3
	vmovdqa [%rax], wi

	// check exit
	cmp %rax, %rcx
	je %=f

	// setup next iter
	add %rax, 16
	vmovdqa wi7, [%rax - (16 * 7)]
	vmovdqa wi2, wlater
	vmovdqa wlater, wi
	vmovdqa wi16, wi15
	vmovdqa wi15, [%rax - (16 * 15)]

	// loop
	jmp %=b
	// loop exit
	%=:

	// setup a..h
	vmovdqa a, [%H]
	vmovdqa b, [%H + (16 * 1)]
	vmovdqa c, [%H + (16 * 2)]
	vmovdqa d, [%H + (16 * 3)]
	vmovdqa e, [%H + (16 * 4)]
	vmovdqa f, [%H + (16 * 5)]
	vmovdqa g, [%H + (16 * 6)]
	vmovdqa h, [%H + (16 * 7)]

	xor %rax, %rax
	lea %rcx, %K
	lea %rdx, [%rsp - 112]

	// loop begin
	%=:
	vbroadcastss ki, [%rcx + %rax]
	vmovdqa wi, [%rdx + 4 * %rax]

	@compress a, b, c, d, e, f, g, h, wi, ki

	// check exit
	cmp %rax, 63 * 4
	je %=f

	add %rax, 4

	// loop end
	jmp %=b
	// loop exit
	%=:

	// do final add
	vpaddd a, a, [%H]
	vpaddd b, b, [%H + 16]
	vpaddd c, c, [%H + 32]
	vpaddd d, d, [%H + 48]
	vpaddd e, e, [%H + 64]
	vpaddd f, f, [%H + 80]
	vpaddd g, g, [%H + 96]
	vpaddd h, h, [%H + 112]

	// rearange data to be stored
	vpunpckldq tmp0, a, b
	vpunpckhdq tmp1, a, b
	vpunpckldq tmp2, c, d
	vpunpckhdq tmp3, c, d

	vpunpcklbw m0, tmp0, tmp2
	vpunpckhbw m1, tmp0, tmp2
	vpunpcklbw m2, tmp1, tmp3
	vpunpckhbw m3, tpm1, tpm3

	vpunpckldq tmp0, e, f
	vpunpckhdq tpm1, e, f
	vpunpckldq tmp2, g, h
	vpunpckhdq tmp3, g, h

	vpunpcklbw m4, tmp0, tmp2
	vpunpckhbw m5, tmp0, tmp2
	vpunpcklbw m6, tmp1, tmp3
	vpuncpkhbw m7, tmp1, tmp3

	vmovdqa mask, [%PunpackMask]

	vpshufb m0, m0, mask
	vpshufb m1, m1, mask
	vpshufb m2, m2, mask
	vpshufb m3, m3, mask
	vpshufb m4, m4, mask
	vpshufb m5, m5, mask
	vpshufb m6, m6, mask
	vpshufb m7, m7, mask

	vmovdqa [%rdi], m0
	vmovdqa [%rdi + 16], m4
	vmovdqa [%rdi + 32], m1
	vmovdqa [%rdi + 48], m5
	vmovdqa [%rdi + 64], m2
	vmovdqa [%rdi + 80], m6
	vmovdqa [%rdi + 96], m3
	vmovdqa [%rdi + 112], m7

	add %rsp 1024 + 8 - 112